{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from CVS\n",
    "The code in this notebook copies the cvs files for beaked whales from S3 to HDFS and then loads the data into \n",
    "a spark DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['split']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): pandas in /usr/local/lib64/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): pytz>=2011k in /usr/local/lib/python2.7/site-packages (from pandas)\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.7.0 in /usr/local/lib64/python2.7/site-packages (from pandas)\n",
      "Requirement already satisfied (use --upgrade to upgrade): python-dateutil in /usr/local/lib/python2.7/site-packages (from pandas)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.5 in /usr/local/lib/python2.7/site-packages (from python-dateutil->pandas)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy in /usr/local/lib64/python2.7/site-packages\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=master_url)\n",
    "\n",
    "from pyspark.sql import Row, SQLContext,DataFrame\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "!pip install pandas\n",
    "!pip install scipy\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from scipy.io import loadmat,savemat,whosmat\n",
    "\n",
    "from string import split\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import shape\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format of cvs files\n",
    "|field name     | Description               | Data Type\n",
    "|---------------|---------------------------|--------------\n",
    "|0: time        | time of click             | string in datetime format `%Y-%m-%d %H:%M:%S.%f`\n",
    "|1: species\t\t| Initial species classification\t        | 'str'\n",
    "|2: site\t\t| name of site\t\t        | 'str'\n",
    "|3: rec_no\t\t| recording number\t\t    | 'str'\n",
    "|4: bout_i\t\t| bout number\t\t        | numpy.int64\n",
    "|5: peak2peak\t| peak to peak magnitude    | \t\t\tnumpy.float64\n",
    "|6: MSN\t        |\twave form |\t\t an array of length 202\n",
    "|208: MSP\t\t|\tspectra |\t an array of length 101  \n",
    "|309: TPWS1\t\t| 1 if click appears in TPWS1\t| \tbool\n",
    "|310: MD1\t\t|\t--- \" ---\tin MD1|\tbool\n",
    "|311: FD1\t    |\t--- \" ---\tin FD1|\tbool\n",
    "|312: TPWS2\t\t| 1 if click appears in TPWS2\t| \tbool\n",
    "|313: MD2\t\t|\t--- \" ---\tin MD2|\tbool\n",
    "|314: FD2\t    |\t--- \" ---\tin FD2|\tbool\n",
    "|315: TPWS3\t\t| 1 if click appears in TPWS3\t| \tbool\n",
    "|316: MD3\t\t|\t--- \" ---\tin MD3|\tbool\n",
    "|317: FD3\t    |\t--- \" ---\tin FD3|\tbool\n",
    "total number of fields= 318\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check  S3 contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd /root/ipython/BeakedWhaleClassification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remember to set credentials under \"setup S3\" in the cluster setup page\n",
    "s3helper.open_bucket('while-classification')\n",
    "s3helper.ls_s3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dirs=s3helper.ls_s3('CVS')\n",
    "dirs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy from S3 to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1=time()\n",
    "s3helper.s3_to_hdfs('CVS', 'CVS')\n",
    "time()-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Read data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('time', 'datetime') {'start': 0, 'parser': <function parse_date at 0x7f8945c98b90>, 'end': 1, 'name': 'time'}\n",
      "('species', 'str') {'start': 1, 'parser': <function parse_string at 0x7f8945972938>, 'end': 2, 'name': 'species'}\n",
      "('site', 'str') {'start': 2, 'parser': <function parse_string at 0x7f8945972938>, 'end': 3, 'name': 'site'}\n",
      "('rec_no', 'str') {'start': 3, 'parser': <function parse_string at 0x7f8945972938>, 'end': 4, 'name': 'rec_no'}\n",
      "('bout_i', 'int') {'start': 4, 'parser': <function parse_int at 0x7f8945972aa0>, 'end': 5, 'name': 'bout_i'}\n",
      "('peak2peak', 'float') {'start': 5, 'parser': <function parse_float at 0x7f8945972c80>, 'end': 6, 'name': 'peak2peak'}\n",
      "('MSN', 'array', 202) {'start': 6, 'parser': <function parse_array at 0x7f89813686e0>, 'end': 208, 'name': 'MSN'}\n",
      "('MSP', 'array', 101) {'start': 208, 'parser': <function parse_array at 0x7f89813686e0>, 'end': 309, 'name': 'MSP'}\n",
      "('TPWS1', 'bool') {'start': 309, 'parser': <function parse_int at 0x7f8945972aa0>, 'end': 310, 'name': 'TPWS1'}\n",
      "('MD1', 'bool') {'start': 310, 'parser': <function parse_int at 0x7f8945972aa0>, 'end': 311, 'name': 'MD1'}\n",
      "('FD1', 'bool') {'start': 311, 'parser': <function parse_int at 0x7f8945972aa0>, 'end': 312, 'name': 'FD1'}\n",
      "('TPWS2', 'bool') {'start': 312, 'parser': <function parse_int at 0x7f8945972aa0>, 'end': 313, 'name': 'TPWS2'}\n",
      "('MD2', 'bool') {'start': 313, 'parser': <function parse_int at 0x7f8945972aa0>, 'end': 314, 'name': 'MD2'}\n",
      "('FD2', 'bool') {'start': 314, 'parser': <function parse_int at 0x7f8945972aa0>, 'end': 315, 'name': 'FD2'}\n",
      "('TPWS3', 'bool') {'start': 315, 'parser': <function parse_int at 0x7f8945972aa0>, 'end': 316, 'name': 'TPWS3'}\n",
      "('MD3', 'bool') {'start': 316, 'parser': <function parse_int at 0x7f8945972aa0>, 'end': 317, 'name': 'MD3'}\n",
      "('FD3', 'bool') {'start': 317, 'parser': <function parse_int at 0x7f8945972aa0>, 'end': 318, 'name': 'FD3'}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-bd65e7302bbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mCVS_Data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/CVS/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCVS_Data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mtextFile\u001b[0;34m(self, name, minPartitions, use_unicode)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34mu'Hello world!'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \"\"\"\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0mminPartitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminPartitions\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m         return RDD(self._jsc.textFile(name, minPartitions), self,\n\u001b[1;32m    475\u001b[0m                    UTF8Deserializer(use_unicode))\n",
      "\u001b[0;32m/root/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mdefaultParallelism\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    346\u001b[0m         reduce tasks)\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('lib')\n",
    "from row_parser import *\n",
    "\n",
    "Parse_rules,field_names,RowObject = init_parser_parameters()\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "CVS_Data=sc.textFile(\"/CVS/\")\n",
    "row=CVS_Data.first()\n",
    "print row\n",
    "\n",
    "def parse(row):\n",
    "    items=row.split(',')\n",
    "    D=[]\n",
    "    for pr in Parse_rules:\n",
    "        start=pr['start']\n",
    "        end=pr['end']\n",
    "        parser=pr['parser']\n",
    "        if end-start==1:\n",
    "            D.append(parser(items[start]))\n",
    "        else:\n",
    "            D.append(parser(items[start:end]))\n",
    "    return RowObject(*D)\n",
    "\n",
    "#parse(row)\n",
    "\n",
    "RDD=CVS_Data.map(parse)\n",
    "# RDD.take(3)\n",
    "\n",
    "df=sqlContext.createDataFrame(RDD)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0=time()\n",
    "print df.cache().count()\n",
    "print time()-t0\n",
    "\n",
    "t0=time()\n",
    "print df.count()\n",
    "time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
