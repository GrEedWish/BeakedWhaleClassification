{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from CVS\n",
    "The code in this notebook copies the cvs files for beaked whales from S3 to HDFS and then loads the data into \n",
    "a spark DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=master_url)\n",
    "\n",
    "from pyspark.sql import Row, SQLContext,DataFrame\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "!pip install pandas\n",
    "!pip install scipy\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from scipy.io import loadmat,savemat,whosmat\n",
    "\n",
    "from string import split\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import shape\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format of cvs files\n",
    "|field name     | Description               | Data Type\n",
    "|---------------|---------------------------|--------------\n",
    "|0: time        | time of click             | string in datetime format `%Y-%m-%d %H:%M:%S.%f`\n",
    "|1: species\t\t| Initial species classification\t        | 'str'\n",
    "|2: site\t\t| name of site\t\t        | 'str'\n",
    "|3: rec_no\t\t| recording number\t\t    | 'str'\n",
    "|4: bout_i\t\t| bout number\t\t        | numpy.int64\n",
    "|5: peak2peak\t| peak to peak magnitude    | \t\t\tnumpy.float64\n",
    "|6: MSN\t        |\twave form |\t\t an array of length 202\n",
    "|208: MSP\t\t|\tspectra |\t an array of length 101  \n",
    "|309: TPWS1\t\t| 1 if click appears in TPWS1\t| \tbool\n",
    "|310: MD1\t\t|\t--- \" ---\tin MD1|\tbool\n",
    "|311: FD1\t    |\t--- \" ---\tin FD1|\tbool\n",
    "|312: TPWS2\t\t| 1 if click appears in TPWS2\t| \tbool\n",
    "|313: MD2\t\t|\t--- \" ---\tin MD2|\tbool\n",
    "|314: FD2\t    |\t--- \" ---\tin FD2|\tbool\n",
    "|315: TPWS3\t\t| 1 if click appears in TPWS3\t| \tbool\n",
    "|316: MD3\t\t|\t--- \" ---\tin MD3|\tbool\n",
    "|317: FD3\t    |\t--- \" ---\tin FD3|\tbool\n",
    "total number of fields= 318\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%writefile lib/parse.py\n",
    "import numpy as np\n",
    "from pyspark.sql import Row, SQLContext,DataFrame\n",
    "from pyspark.sql.types import *\n",
    "import datetime as dt\n",
    "\n",
    "class row_parser:\n",
    "    date_format='%Y-%m-%d %H:%M:%S.%f'\n",
    "    Fields=[('time', 'datetime'),\n",
    "        ('species', 'str'),\n",
    "        ('site', 'str'),\n",
    "        ('rec_no', 'str'),\n",
    "        ('bout_i', 'int'),\n",
    "        ('peak2peak', 'float'),\n",
    "        ('MSN', 'array',202),\n",
    "        ('MSP', 'array',101),\n",
    "        ('TPWS1', 'bool'),\n",
    "        ('MD1', 'bool'),\n",
    "        ('FD1', 'bool'),\n",
    "        ('TPWS2', 'bool'),\n",
    "        ('MD2', 'bool'),\n",
    "        ('FD2', 'bool'),\n",
    "        ('TPWS3', 'bool'),\n",
    "        ('MD3', 'bool'),\n",
    "        ('FD3', 'bool')]\n",
    "\n",
    "    def __init__():\n",
    "        #prepare date structure for parsing\n",
    "        self.Parse_rules=[]\n",
    "        index=0\n",
    "        for field in Fields:\n",
    "            _type=field[1]\n",
    "            #print _type\n",
    "            _len=1 # default length in terms of csv fields\n",
    "            if _type =='array': \n",
    "                parser=parse_array\n",
    "                _len=int(field[2])\n",
    "            elif _type=='datetime': \n",
    "                parser=parse_date\n",
    "            elif _type=='int': \n",
    "                parser=parse_int\n",
    "            elif _type=='float': \n",
    "                parser=parse_float\n",
    "            elif _type=='bool': \n",
    "                parser=parse_int\n",
    "            elif _type=='str': \n",
    "                parser=parse_string\n",
    "            else:\n",
    "                print 'unrecognized type',_type\n",
    "            rule={'name':field[0],\n",
    "                  'start':index,\n",
    "                  'end':index+_len,\n",
    "                  'parser':parser}\n",
    "            print field,rule\n",
    "            self.Parse_rules.append(rule)\n",
    "            index+=_len\n",
    "\n",
    "        self.field_names=[a['name'] for a in self.Parse_rules]\n",
    "        self.RowObject= Row(*self.field_names)\n",
    "\n",
    "\n",
    "    def parse_date(s):\n",
    "        #print 'date string=\"%s\"'%s\n",
    "        return dt.datetime.strptime(s,date_format)\n",
    "    def parse_array(a):\n",
    "        np_array=np.array([np.float64(x) for x in a])\n",
    "        return packArray(np_array)\n",
    "    def parse_int(s):\n",
    "        return int(s)\n",
    "    def parse_float(s):\n",
    "        return float(s)\n",
    "    def parse_string(s):\n",
    "        return(s)\n",
    "\n",
    "    def packArray(a):\n",
    "        if type(a)!=np.ndarray:\n",
    "            raise Exception(\"input to packArray should be numpy.ndarray. It is instead \"+str(type(a)))\n",
    "        return bytearray(a.tobytes())\n",
    "    def unpackArray(x,data_type=np.int16):\n",
    "        return np.frombuffer(x,dtype=data_type)\n",
    "\n",
    "\n",
    "    def parse(row):\n",
    "        items=row.split(',')\n",
    "        D=[]\n",
    "        for pr in self.Parse_rules:\n",
    "            start=pr['start']\n",
    "            end=pr['end']\n",
    "            parser=pr['parser']\n",
    "            if end-start==1:\n",
    "                D.append(parser(items[start]))\n",
    "            else:\n",
    "                D.append(parser(items[start:end]))\n",
    "        return self.RowObject(*D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/ipython/BeakedWhaleClassification\n"
     ]
    }
   ],
   "source": [
    "%cd /root/ipython/BeakedWhaleClassification/\n",
    "%run /root/Credentials.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3helper.set_credential(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'CVS',\n",
       " u'DT_Cuviers',\n",
       " u'DT_Gervais',\n",
       " u'GC_Cuviers',\n",
       " u'GC_Gervais',\n",
       " u'MC_Cuviers',\n",
       " u'MC_Gervais']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3helper.open_bucket('while-classification')\n",
    "s3helper.ls_s3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'CVS/DT.01.Cuviers',\n",
       " u'CVS/DT.01.Gervais',\n",
       " u'CVS/DT.02.Cuviers',\n",
       " u'CVS/DT.02.Gervais',\n",
       " u'CVS/DT.03.Cuviers',\n",
       " u'CVS/DT.03.Gervais',\n",
       " u'CVS/DT.04.Cuviers',\n",
       " u'CVS/DT.04.Gervais',\n",
       " u'CVS/DT.05.Cuviers',\n",
       " u'CVS/DT.05.Gervais']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs=s3helper.ls_s3('CVS')\n",
    "dirs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy from S3 to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487.370493888855"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1=time()\n",
    "s3helper.s3_to_hdfs('CVS', 'CVS')\n",
    "time()-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Read data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "CVS_Data=sc.textFile(\"/CVS/\")\n",
    "RDD=CVS_Data.map(parse)\n",
    "\n",
    "df=sqlContext.createDataFrame(RDD)\n",
    "\n",
    "t0=time()\n",
    "print df.cache().count()\n",
    "print time()-t0\n",
    "\n",
    "t0=time()\n",
    "print df.count()\n",
    "time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6353182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8903360366821289"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
