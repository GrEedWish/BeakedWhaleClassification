{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from CVS\n",
    "The code in this notebook copies the cvs files for beaked whales from S3 to HDFS and then loads the data into \n",
    "a spark DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=master_url, pyFiles=['lib/numpy_pack.py','lib/row_parser.py','lib/spark_PCA.py'])\n",
    "\n",
    "from pyspark.sql import Row, SQLContext,DataFrame\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip\n",
    "\n",
    "#!sudo /usr/local/bin/pip install pandas\n",
    "\n",
    "#!sudo /usr/local/bin/pip install scipy\n",
    "!sudo /usr/local/bin/pip install --upgrade numpy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload # use this magic to reload modules\n",
    "\n",
    "#import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "#import scipy\n",
    "#from scipy.io import loadmat,savemat,whosmat\n",
    "\n",
    "from string import split\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import shape\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "print 'numpy version=',np.__version__, 'should be > 1.10'\n",
    "#print 'scipy version=',scipy.__version__\n",
    "#print 'pandas version=',pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format of cvs files\n",
    "|field name     | Description               | Data Type\n",
    "|---------------|---------------------------|--------------\n",
    "|0: time        | time of click             | string in datetime format `%Y-%m-%d %H:%M:%S.%f`\n",
    "|1: species\t\t| Initial species classification\t        | 'str'\n",
    "|2: site\t\t| name of site\t\t        | 'str'\n",
    "|3: rec_no\t\t| recording number\t\t    | 'str'\n",
    "|4: bout_i\t\t| bout number\t\t        | numpy.int64\n",
    "|5: peak2peak\t| peak to peak magnitude    | \t\t\tnumpy.float64\n",
    "|6: MSN\t        |\twave form |\t\t an array of length 202\n",
    "|208: MSP\t\t|\tspectra |\t an array of length 101  \n",
    "|309: TPWS1\t\t| 1 if click appears in TPWS1\t| \tbool\n",
    "|310: MD1\t\t|\t--- \" ---\tin MD1|\tbool\n",
    "|311: FD1\t    |\t--- \" ---\tin FD1|\tbool\n",
    "|312: TPWS2\t\t| 1 if click appears in TPWS2\t| \tbool\n",
    "|313: MD2\t\t|\t--- \" ---\tin MD2|\tbool\n",
    "|314: FD2\t    |\t--- \" ---\tin FD2|\tbool\n",
    "|315: TPWS3\t\t| 1 if click appears in TPWS3\t| \tbool\n",
    "|316: MD3\t\t|\t--- \" ---\tin MD3|\tbool\n",
    "|317: FD3\t    |\t--- \" ---\tin FD3|\tbool\n",
    "total number of fields= 318\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload # use this magic to reload modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check  S3 contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%cd /root/ipython/BeakedWhaleClassification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remember to set credentials under \"setup S3\" in the cluster setup page\n",
    "s3helper.open_bucket('while-classification')\n",
    "s3helper.ls_s3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dirs=s3helper.ls_s3('CVS')\n",
    "dirs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy from S3 to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1=time()\n",
    "s3helper.s3_to_hdfs('CVS', 'CVS')\n",
    "time()-t1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Read data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load lib/row_parser.py\n",
    "from pyspark.sql import Row, SQLContext,DataFrame\n",
    "from pyspark.sql.types import *\n",
    "import datetime as dt\n",
    "\n",
    "def packArray(a):\n",
    "    if type(a)!=np.ndarray:\n",
    "        raise Exception(\"input to packArray should be numpy.ndarray. It is instead \"+str(type(a)))\n",
    "    return bytearray(a.tobytes())\n",
    "def unpackArray(x,data_type=np.int16):\n",
    "    return np.frombuffer(x,dtype=data_type)\n",
    "\n",
    "def init_parser_parameters():\n",
    "    def parse_date(s):\n",
    "        return dt.datetime.strptime(s,'%Y-%m-%d %H:%M:%S.%f')\n",
    "    def parse_array(a):\n",
    "        np_array=np.array([np.float64(x) for x in a])\n",
    "        return packArray(np_array)\n",
    "    def parse_int(s):\n",
    "        return int(s)\n",
    "    def parse_float(s):\n",
    "        return float(s)\n",
    "    def parse_string(s):\n",
    "        return(s)\n",
    "\n",
    "    Fields=[('time', 'datetime'),\n",
    "        ('species', 'str'),\n",
    "        ('site', 'str'),\n",
    "        ('rec_no', 'str'),\n",
    "        ('bout_i', 'int'),\n",
    "        ('peak2peak', 'float'),\n",
    "        ('MSN', 'array',202),\n",
    "        ('MSP', 'array',101),\n",
    "        ('TPWS1', 'bool'),\n",
    "        ('MD1', 'bool'),\n",
    "        ('FD1', 'bool'),\n",
    "        ('TPWS2', 'bool'),\n",
    "        ('MD2', 'bool'),\n",
    "        ('FD2', 'bool'),\n",
    "        ('TPWS3', 'bool'),\n",
    "        ('MD3', 'bool'),\n",
    "        ('FD3', 'bool')]\n",
    "\n",
    "    #global Parse_rules, RowObject\n",
    "    #prepare date structure for parsing\n",
    "    Parse_rules=[]\n",
    "    index=0\n",
    "    for field in Fields:\n",
    "        _type=field[1]\n",
    "        #print _type\n",
    "        _len=1 # default length in terms of csv fields\n",
    "        if _type =='array': \n",
    "            parser=parse_array\n",
    "            _len=int(field[2])\n",
    "        elif _type=='datetime': \n",
    "            parser=parse_date\n",
    "        elif _type=='int': \n",
    "            parser=parse_int\n",
    "        elif _type=='float': \n",
    "            parser=parse_float\n",
    "        elif _type=='bool': \n",
    "            parser=parse_int\n",
    "        elif _type=='str': \n",
    "            parser=parse_string\n",
    "        else:\n",
    "            print 'unrecognized type',_type\n",
    "        rule={'name':field[0],\n",
    "              'start':index,\n",
    "              'end':index+_len,\n",
    "              'parser':parser}\n",
    "        print field,rule\n",
    "        Parse_rules.append(rule)\n",
    "        index+=_len\n",
    "\n",
    "    field_names=[a['name'] for a in Parse_rules]\n",
    "    RowObject= Row(*field_names)\n",
    "    return Parse_rules,field_names,RowObject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('lib')\n",
    "from row_parser import *\n",
    "\n",
    "Parse_rules,field_names,RowObject = init_parser_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(row):\n",
    "    #Parse_rules,field_names,RowObject = parser_data.value\n",
    "    items=row.split(',')\n",
    "    D=[]\n",
    "    for pr in Parse_rules:\n",
    "        start=pr['start']\n",
    "        end=pr['end']\n",
    "        parser=pr['parser']\n",
    "        if end-start==1:\n",
    "            D.append(parser(items[start]))\n",
    "        else:\n",
    "            D.append(parser(items[start:end]))\n",
    "    return RowObject(*D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "t1=time()\n",
    "CVS_Data=sc.textFile(\"/CVS/\")\n",
    "CVS_Data.cache().count()\n",
    "print time()-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row=CVS_Data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print 'a row:\\n',row\n",
    "\n",
    "parse(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RDD=CVS_Data.map(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=sqlContext.createDataFrame(RDD)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0=time()\n",
    "print df.cache().count()\n",
    "print time()-t0\n",
    "\n",
    "t0=time()\n",
    "print df.count()\n",
    "time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "5.*350/60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A=df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D=A.asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t=0\n",
    "for k in D.keys():\n",
    "    t+=size(D[k])\n",
    "    print k, size(D[k]),t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "2439.*6353182./1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "from string import split\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import shape\n",
    "\n",
    "from glob import glob\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from row_parser import unpackArray\n",
    "import numpy\n",
    "def g(row):\n",
    "    return unpackArray(row[field],data_type=numpy.float64)\n",
    "def unpackArray(x,data_type=numpy.int16):\n",
    "    return numpy.frombuffer(x,dtype=data_type)\n",
    "L=df.take(20)\n",
    "field='MSP'\n",
    "for a in L:\n",
    "    plot(g(a))\n",
    "title(field);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "field='MSN'\n",
    "for a in L:\n",
    "    plot(g(a))\n",
    "title(field);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from spark_PCA import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unpack(bytearray):\n",
    "    return unpackArray(bytearray,data_type=numpy.float64)\n",
    "feature='MSP'\n",
    "rdd=df.rdd.map(g)\n",
    "print type(rdd)\n",
    "rdd.first()[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0=time()\n",
    "rdd.cache().count()\n",
    "print time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0=time()\n",
    "COV=computeCov(rdd)\n",
    "print time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
